üß† RNN (Recurrent Neural Networks)

RNNs are designed for sequence data, where order matters.
Examples:
Text (sentences, reviews, chat)
Time series (stock prices, weather)
Speech/audio
Video frames
CNN ‚Üí Good for images
RNN ‚Üí Good for sequences

‚≠ê 1. Why Normal Neural Networks Fail for Sequences
Traditional neural networks (MLP) think all inputs are independent.
But sequences have memory.
Example:
Sentence: ‚ÄúI am going to the bank to withdraw cash.‚Äù
The meaning of "bank" depends on previous words.
MLP cannot remember this.
RNN can.

‚≠ê 2. What Makes RNN Special?
RNN has a loop inside.
This loop lets it remember past information.
Formula:
h_t = activation(Wx_t + Uh_(t-1) + b)
Where:
x_t ‚Üí input at time step t
h_(t-1) ‚Üí previous hidden state
h_t ‚Üí current memory
This ‚Äúhidden state‚Äù carries memory across time.

‚≠ê 3. Visual Explanation
RNN unfolds like this:
x1 --> [RNN] --> h1
x2 --> [RNN] --> h2
x3 --> [RNN] --> h3
At each step:
It reads input
It remembers previous hidden state
It produces output

‚≠ê 4. Intuition (Very Simple)
Imagine reading a sentence word by word.
You don‚Äôt forget the previous words when reading the next.
RNN works the same.

‚≠ê 5. The Problem: Vanishing Gradient
RNN has a major issue:
üëâ When sequences are long, gradients become extremely small.
This means the network forgets what happened earlier.
That‚Äôs why RNN struggles with:
long sentences
long time-series
Solution ‚Üí LSTM & GRU (Topic 7)

‚≠ê 6. Where RNN is Still Useful
Short text (sentences 5‚Äì10 words)
Simple time-series
Predict next number in sequence
Character-level models (generate text character by character)
But now LSTMs and Transformers are much better.

‚≠ê 8. Real Use Case Example
Sentiment analysis (positive/negative)
Input sentence:
"I love this movie"
RNN processes:
I
love
this
movie
It keeps updating memory after each word ‚Üí final state gives sentiment.

‚úÖ Hidden State in RNN:
The hidden state in an RNN acts like the model‚Äôs memory.
It:
remembers information from previous time steps
carries context from earlier inputs
helps the RNN understand the sequence instead of treating each input separately
So at each step:
new_hidden = function(current_input + previous_hidden)
This is how an RNN knows what came before in a sentence or a time series.