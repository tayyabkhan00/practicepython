ğŸ§  TOPIC 2 â€” Activation Functions:

Activation functions decide whether a neuron should fire or not.
They add non-linearity so neural networks can learn complex patterns.
Without them â†’ your neural network becomes just linear regression.

â­ 1. Sigmoid:
Used for: Binary classification (Yes/No, True/False)
Range:
0 â†’ 1
Idea:
Turns numbers into probabilities.
Formula:
output = 1 / (1 + e^(-x))
Visual meaning:
Large positive â†’ close to 1
Large negative â†’ close to 0

â­ 2. ReLU (Rectified Linear Unit)
Used in: Almost every modern neural network (CNNs especially)
Range:
0 â†’ âˆ
Formula:
ReLU(x) = max(0, x)
Idea:
If value < 0 â†’ becomes 0
If value > 0 â†’ stays same
Why used so much?
âœ” Fast
âœ” Prevents vanishing gradients
âœ” Gives better performance

â­ 3. Tanh (Hyperbolic Tangent)
Used in: RNNs, LSTMs
Range:
-1 â†’ 1
Better than sigmoid because outputs are centered around 0.

â­ 4. Softmax
Used for: Multiclass classification
Example: classify image into 10 classes.
What it does:
Turns numbers â†’ probabilities that add to 1.
Example:
[2.1, 1.0, 0.1] â†’ [0.70, 0.25, 0.05]

â­ When to use which?
Task	                Activation
Binary classification	Sigmoid
Hidden layers	        ReLU
RNN/LSTM networks	    Tanh
Multiclass output	    Softmax