ğŸ§  In LSTM, we have TWO types of memory

h_t â†’ Hidden state (short-term memory)
c_t â†’ Cell state (long-term memory)
Both are updated at every timestep.

â­ Is hâ‚™ the previous or recent memory?
h_n = the final hidden state after the last time step.
It represents:
short-term memory
what LSTM wants to â€œoutputâ€ at each step
So hâ‚™ = recent (latest) memory.

â­ How is câ‚™ calculated?
c_n = final cell state after the last time step
This is the long-term memory and is computed using the LSTM formula:
c_t = f_t * c_(t-1) + i_t * g_t
Where:
f_t = forget gate
i_t = input gate
g_t = candidate new memory
c_(t-1) = previous cell state
Letâ€™s break this into simple words:
âœ” Step 1: Forget old memory
forget_gate * previous_cell_state
âœ” Step 2: Add new important memory
input_gate * candidate_state
âœ” Step 3: Combine to form new cell state
c_t = (things to keep) + (things to add)
This updated c_t becomes the next timestepâ€™s c_(t-1).
At the final timestep:
c_n = last cell state (long-term memory)

â­ Relationship between hâ‚œ and câ‚œ
After computing c_t, the hidden state is calculated using the output gate:
h_t = output_gate * tanh(c_t)
So:
c_t = what LSTM stores
h_t = what LSTM outputs

ğŸ“Œ Simple Intuition
Memory Type.            Variable	            Meaning
Long-term memory	        câ‚™          	What the LSTM keeps for a long time
Short-term output memory	hâ‚™	            What the LSTM uses right now

âœ” Example in PyTorch Returning Values
If sequence length = 5, LSTM returns:
output â†’ hidden states at all 5 steps
h_n    â†’ hidden state at step 5 (final)
c_n    â†’ cell state at step 5 (final)
So yes:
hâ‚™ = recent short-term memory
câ‚™ = recent long-term memory
And c_n is ALWAYS calculated using:
forget gate + new info gate

ğŸ§  Understanding Each Gate:

ğŸ”¹ 1. Forget Gate (What to remove?)
Symbol: f_t
Activation: Sigmoid (values between 0 and 1)
0 â†’ forget completely
1 â†’ keep completely
Purpose:
Decides which part of the old memory (c_{t-1}) to erase.
âœ” Example:
Sentence: â€œThe dog I saw yesterday was huge.â€
For sentiment analysis â†’ "yesterday" gets forgotten.

ğŸ”¹ 2. Input Gate (What to add?)
Symbol: i_t
Activation: Sigmoid
Along with the candidate memory (Ä‰_t), it decides what new information is added to the cell state.
âœ” Example:
Sentence: â€œhugeâ€ â†’ important word â†’ input gate adds this to memory.

ğŸ”¹ 3. Output Gate (What to show?)
Symbol: o_t
Activation: Sigmoid
Controls what part of the cell state becomes the hidden state (h_t) output.
âœ” Example:
At the end of reading a sentence â†’ output gate decides the final sentiment vector.

ğŸŒŸ Combined LSTM Cell Visualization
This image shows:
Forget gate (left)
Input gate (middle)
Output gate (right)
Cell state flowing through
Hidden state being produced