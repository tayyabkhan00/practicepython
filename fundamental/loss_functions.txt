üß† Loss Functions 

A loss function tells the neural network how wrong it is.
During training:
The network makes a prediction
Loss function compares prediction vs correct answer
If prediction is wrong ‚Üí loss is high
If prediction is correct ‚Üí loss is low
Backpropagation uses this loss to adjust weights
Loss = ‚Äúhow bad is your prediction?‚Äù
Lower loss = better model.

‚≠ê The 3 Most Important Loss Functions
1Ô∏è‚É£ MSE ‚Äî Mean Squared Error
Used for ‚Üí Regression (predicting numbers)
Formula:
MSE = average((prediction - actual)^2)
Idea:
Square the errors so big mistakes are punished more.
Example:
Predictions: [4, 5]
Actual: [3, 3]
Errors:
(4‚Äì3)¬≤ = 1
(5‚Äì3)¬≤ = 4
MSE = (1 + 4) / 2 = 2.5
2Ô∏è‚É£ Binary Cross Entropy (BCE):
Used for ‚Üí Binary Classification
Examples:
spam / not spam
dog / not dog
positive / negative
Formula (you don‚Äôt need to memorize):
loss = - [ y*log(p) + (1-y)*log(1-p) ]
Intuition:
If model predicts correctly ‚Üí small loss
If model is confident but wrong ‚Üí HUGE loss
Example:
Actual (y) = 1
Predicted probability (p) = 0.9
Loss = very small ‚Üí good
If p = 0.1 ‚Üí loss huge ‚Üí bad
3Ô∏è‚É£ Categorical Cross Entropy (CCE)
Used for ‚Üí Multi-class classification
Examples:
10 digits (MNIST)
ImageNet 1000 classes
News topics (sports, politics, tech‚Ä¶)
Works with Softmax.
Example:
True class = ‚Äúcat‚Äù
Model outputs:
cat ‚Üí 0.80  
dog ‚Üí 0.15  
cow ‚Üí 0.05
Loss = low (good)
If model predicted:
cat ‚Üí 0.05  
dog ‚Üí 0.60  
cow ‚Üí 0.35
Loss = high (bad)

‚≠ê Table Summary (Very Important)
Task	                     Loss Function	             Output Activation        PyTorch Function
Regression	                 MSE	                        Linear                 nn.MSELoss()
Binary classification	     Binary Cross Entropy	        Sigmoid                nn.BCELoss()
Multi-class classification	 Categorical Cross Entropy	    Softmax                nn.CrossEntropyLoss()

